{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KuOhiceP4ezH",
        "outputId": "75bb8fb3-b165-4e15-fa84-e7a5d9d4909a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     /Users/nakatayuki/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('punkt') # https://www.nltk.org/api/nltk.tokenize.punkt.html\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "stemmer = SnowballStemmer('english')\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.spatial.distance import squareform, pdist\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import gensim\n",
        "import logging\n",
        "logging.basicConfig(format='(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import lightgbm as lgb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuvsFvYnYGCV"
      },
      "source": [
        "## Import Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "oBNCjV3m5a4V"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv(f'data/train.csv')\n",
        "test_df = pd.read_csv(f'data/test.csv')\n",
        "sample_submit_df = pd.read_csv(f'data/sample_submit.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "k5ktVjVT5-bm"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "特定の記号を空白に置換し、英数字以外を除去して小文字に統一するためのテキストクリーニング関数\n",
        "\"\"\"\n",
        "def clean_text(text):\n",
        "    list_of_cleaning_signs = ['\\x0c', '\\n']\n",
        "    for sign in list_of_cleaning_signs:\n",
        "        text = text.replace(sign, ' ')\n",
        "\n",
        "    # `re.sub()` では\n",
        "    # 第一引数に正規表現パターン、\n",
        "    # 第二引数に置換先文字列、\n",
        "    # 第三引数に処理対象の文字列を指定する。\n",
        "    clean_text = re.sub('[^a-zA-Z]+', ' ', text)\n",
        "    return clean_text.lower()\n",
        "\n",
        "\"\"\"\n",
        "文にトークン化し、さらに単語にトークン化して、\n",
        "英字のみをフィルタし、それぞれの単語を基本形に変換する\n",
        "\n",
        "re.searchの利用例：\n",
        "\n",
        "```\n",
        "s = 'aaa@xxx.com bbb@yyy.net ccc@zzz.org'\n",
        "\n",
        "print(re.search(r'[a-z]+@[a-z]+\\.net', s))\n",
        "# <re.Match object; span=(12, 23), match='bbb@yyy.net'>\n",
        "\n",
        "print(re.search(r'[a-z]+@[a-z]+\\.[a-z]+', s))\n",
        "# <re.Match object; span=(0, 11), match='aaa@xxx.com'>\n",
        "```\n",
        "\n",
        "\"\"\"\n",
        "def tokenize_and_stem(text):\n",
        "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
        "    filtered_tokens = []\n",
        "    for token in tokens:\n",
        "        if re.search('[a-zA-Z]', token):\n",
        "            filtered_tokens.append(token)\n",
        "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
        "    return stems\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "TF-IDFスコアが高い上位N個の特徴を選択する\n",
        "\"\"\"\n",
        "def top_tfidf_feats(row, terms, top_n=25):\n",
        "    top_ids = np.argsort(row)[::-1][:top_n]\n",
        "    top_feats = [terms[i] for i in top_ids]\n",
        "    return top_feats\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "テキストからTF-IDFによるキーワード抽出を行う。\n",
        "ここでトークナイザーとしてステミング関数を使用\n",
        "\"\"\"\n",
        "def extract_tfidf_keywords(texts, top_n=25):\n",
        "    tfidf_vectorizer = TfidfVectorizer(\n",
        "        max_df=0.95, max_features=2000000,\n",
        "        min_df=0.05, stop_words=\"english\",\n",
        "        use_idf=True, tokenizer=tokenize_and_stem,\n",
        "        ngram_range=(1,3)\n",
        "    )\n",
        "\n",
        "    tfidf_matrix = tfidf_vectorizer.fit_transform(texts)\n",
        "    terms = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "    arr = []\n",
        "    for i in range(0, tfidf_matrix.shape[0]):\n",
        "        row = np.squeeze(tfidf_matrix[i].toarray())\n",
        "        feats = top_tfidf_feats(row, terms, top_n)\n",
        "        arr.append(feats)\n",
        "    return arr, tfidf_vectorizer\n",
        "\n",
        "\n",
        "def create_tfidf_features_df(text):\n",
        "    tfidf_vectorizer = TfidfVectorizer(\n",
        "        max_df=0.95, max_features=2000000,\n",
        "        min_df=0.05, stop_words=\"english\",\n",
        "        use_idf=True, tokenizer=tokenize_and_stem,\n",
        "        ngram_range=(1,3)\n",
        "    )\n",
        "\n",
        "    tfidf_matrix = tfidf_vectorizer.fit_transform(papers_data['Abstract_clean'])\n",
        "    terms = tfidf_vectorizer.get_feature_names_out()\n",
        "    df_features = pd.DataFrame(\n",
        "        tfidf_matrix.toarray(),\n",
        "        columns=tfidf_vectorizer.get_feature_names_out()\n",
        "    )\n",
        "\n",
        "    return df_features\n",
        "\n",
        "def document_vector(doc):\n",
        "    \"\"\"文書に含まれる単語のベクトルの平均を計算\"\"\"\n",
        "    return np.mean([word2vec_model.wv[word] for word in doc if word in word2vec_model.wv], axis=0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ynr7ro7LNXPI"
      },
      "source": [
        "## Pre-Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1SOHG1CTe74w",
        "outputId": "87e08581-3658-401b-dd45-d62ab1979d6b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Volumes/Extreme SSD/Projects/my-projects/ml_compe/.venv/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/Volumes/Extreme SSD/Projects/my-projects/ml_compe/.venv/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'forti', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'mani', 'meanwhil', 'moreov', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'togeth', 'twelv', 'twenti', 'veri', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
            "  warnings.warn(\n",
            "/Volumes/Extreme SSD/Projects/my-projects/ml_compe/.venv/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/Volumes/Extreme SSD/Projects/my-projects/ml_compe/.venv/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'forti', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'mani', 'meanwhil', 'moreov', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'togeth', 'twelv', 'twenti', 'veri', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "papers_data = train_df\n",
        "\n",
        "papers_data['abstract'] = papers_data['abstract'].fillna('')\n",
        "\n",
        "# clean text\n",
        "papers_data['Title_clean'] = papers_data['title'].apply(lambda x:clean_text(x))\n",
        "papers_data['Abstract_clean'] = papers_data['abstract'].apply(lambda x:clean_text(x))\n",
        "\n",
        "# tf-idf\n",
        "abstract2kw, abstract2kw_vectorizer = extract_tfidf_keywords(papers_data['Abstract_clean'], 20)\n",
        "title2kw, title2kw_vectorizer = extract_tfidf_keywords(papers_data['Title_clean'], 20)\n",
        "\n",
        "# 文書のベクトル表現を特徴量化する\n",
        "abstract2kw_features_df = create_tfidf_features_df(papers_data['Abstract_clean'])\n",
        "title2kw_features_df = create_tfidf_features_df(papers_data['Title_clean'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "KEotIZ3x7iWz"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "(asctime)s : INFO : collecting all words and their counts\n",
            "(asctime)s : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "(asctime)s : INFO : PROGRESS: at sentence #10000, processed 200000 words, keeping 324 word types\n",
            "(asctime)s : INFO : PROGRESS: at sentence #20000, processed 400000 words, keeping 324 word types\n",
            "(asctime)s : INFO : PROGRESS: at sentence #30000, processed 600000 words, keeping 324 word types\n",
            "(asctime)s : INFO : PROGRESS: at sentence #40000, processed 800000 words, keeping 324 word types\n",
            "(asctime)s : INFO : PROGRESS: at sentence #50000, processed 1000000 words, keeping 324 word types\n",
            "(asctime)s : INFO : collected 324 word types from a corpus of 1085800 raw words and 54290 sentences\n",
            "(asctime)s : INFO : Creating a fresh vocabulary\n",
            "(asctime)s : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 324 unique words (100.00% of original 324, drops 0)', 'datetime': '2024-02-14T18:47:56.596090', 'gensim': '4.3.2', 'python': '3.9.1 (default, Jan  4 2021, 00:07:59) \\n[Clang 11.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
            "(asctime)s : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 1085800 word corpus (100.00% of original 1085800, drops 0)', 'datetime': '2024-02-14T18:47:56.597520', 'gensim': '4.3.2', 'python': '3.9.1 (default, Jan  4 2021, 00:07:59) \\n[Clang 11.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
            "(asctime)s : INFO : deleting the raw counts dictionary of 324 items\n",
            "(asctime)s : INFO : sample=0.001 downsamples 45 most-common words\n",
            "(asctime)s : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 573044.127435787 word corpus (52.8%% of prior 1085800)', 'datetime': '2024-02-14T18:47:56.600656', 'gensim': '4.3.2', 'python': '3.9.1 (default, Jan  4 2021, 00:07:59) \\n[Clang 11.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
            "(asctime)s : INFO : estimated required memory for 324 words and 100 dimensions: 421200 bytes\n",
            "(asctime)s : INFO : resetting layer weights\n",
            "(asctime)s : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-02-14T18:47:56.605616', 'gensim': '4.3.2', 'python': '3.9.1 (default, Jan  4 2021, 00:07:59) \\n[Clang 11.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'build_vocab'}\n",
            "(asctime)s : INFO : Word2Vec lifecycle event {'msg': 'training model with 4 workers on 324 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2024-02-14T18:47:56.606037', 'gensim': '4.3.2', 'python': '3.9.1 (default, Jan  4 2021, 00:07:59) \\n[Clang 11.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'train'}\n",
            "(asctime)s : INFO : EPOCH 0: training on 1085800 raw words (573049 effective words) took 0.4s, 1574316 effective words/s\n",
            "(asctime)s : INFO : EPOCH 1: training on 1085800 raw words (572791 effective words) took 0.3s, 2077424 effective words/s\n",
            "(asctime)s : INFO : EPOCH 2: training on 1085800 raw words (572914 effective words) took 0.3s, 2128368 effective words/s\n",
            "(asctime)s : INFO : EPOCH 3: training on 1085800 raw words (573176 effective words) took 0.3s, 1823743 effective words/s\n",
            "(asctime)s : INFO : EPOCH 4: training on 1085800 raw words (572756 effective words) took 0.3s, 1902736 effective words/s\n",
            "(asctime)s : INFO : Word2Vec lifecycle event {'msg': 'training on 5429000 raw words (2864686 effective words) took 1.5s, 1853062 effective words/s', 'datetime': '2024-02-14T18:47:58.152301', 'gensim': '4.3.2', 'python': '3.9.1 (default, Jan  4 2021, 00:07:59) \\n[Clang 11.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'train'}\n",
            "(asctime)s : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=324, vector_size=100, alpha=0.025>', 'datetime': '2024-02-14T18:47:58.152849', 'gensim': '4.3.2', 'python': '3.9.1 (default, Jan  4 2021, 00:07:59) \\n[Clang 11.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'created'}\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "step two:\n",
        "word2vec representation\n",
        "\"\"\"\n",
        "word2vec_model = gensim.models.Word2Vec(abstract2kw+title2kw, vector_size=100, window=5, workers=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "wKHR6SRW7xAN"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "step three:\n",
        "average top-n keywords vectors and compute similarities\n",
        "\n",
        "このコードは、論文の抽象要旨（abstract）とタイトル（title）から抽出したキーワードを用いて、各論文のベクトル表現（doc2vecs）を生成し、それらの間の類似度を計算しています。プロセスは以下のステップに分けられます：\n",
        "\n",
        "1. キーワードベクトルの平均化:\n",
        "  - 各論文について、抽象要旨とタイトルから抽出したキーワードリスト（abstract2kwとtitle2kw）があります。\n",
        "  - それぞれのキーワードについて、gensimのWord2Vecモデル（word2vec_model）を使用して、単語の埋め込みベクトルを取得します。このモデルは、単語を多次元空間上のベクトルとして表現し、単語の意味的な類似性を捉えます。\n",
        "  - キーワードがWord2Vecモデルの語彙に含まれている場合、そのベクトルを取得し、論文のベクトル表現を計算するために使用します。論文のベクトル表現は、抽象要旨とタイトルのキーワードベクトルの平均で表されます。\n",
        "  - 初期ベクトル（vec）は、100次元のゼロベクトルで始まり、キーワードのベクトルが見つかるたびに加算されます。この例では、ベクトルの次元数が100であると仮定していますが、実際の次元数はWord2Vecモデルによって異なります。\n",
        "\n",
        "2. 類似度の計算:\n",
        "  - すべての論文についてベクトル表現を計算した後、scipyライブラリのpdist関数を使用して、論文間のコサイン類似度を計算します。pdist関数は、与えられたベクトルのペア間の距離（この場合は類似度の逆）を計算します。\n",
        "  - squareform関数を使用して、pdistから得られる距離ベクトルを距離行列に変換します。この行列の各要素は、対応する論文ペア間の類似度を表します。\n",
        "  - このコードの主な目的は、論文の内容を基にして類似した論文を特定することです。キーワードの意味的な情報を利用することで、論文のトピックや内容が似ている度合いを定量的に評価することができます。\n",
        "\"\"\"\n",
        "\n",
        "doc2vecs = []\n",
        "for i in range(len(abstract2kw)):\n",
        "    vec = np.zeros(100)  # ベクトルの初期化をNumPy配列で行う\n",
        "    for word in abstract2kw[i]:\n",
        "        if word in word2vec_model.wv.key_to_index:  # 単語がモデルの語彙に含まれているかチェック\n",
        "            vec += word2vec_model.wv[word]  # 単語ベクトルを加算\n",
        "\n",
        "    for word in title2kw[i]:\n",
        "        if word in word2vec_model.wv.key_to_index:  # 単語がモデルの語彙に含まれているかチェック\n",
        "            vec += word2vec_model.wv[word]  # 単語ベクトルを加算\n",
        "\n",
        "    doc2vecs.append(vec / (len(abstract2kw[i]) + len(title2kw[i])))  # 平均ベクトルを計算してリストに追加\n",
        "\n",
        "similarities = squareform(pdist(doc2vecs, 'cosine'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "GFpyZgrefpFR"
      },
      "outputs": [],
      "source": [
        "# word2vecsの出力を特徴量として利用するためにDataframeにする\n",
        "df_features = pd.DataFrame(doc2vecs)\n",
        "\n",
        "# 全てのdataframeを結合する\n",
        "all_features_df = pd.concat([df_features, abstract2kw_features_df, title2kw_features_df], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWpnxXh_gGWw"
      },
      "source": [
        "## LightGBM実装 with train Data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "uoEmI6XHj87Z"
      },
      "outputs": [],
      "source": [
        "# 元データと、論文のベクトル化した値とtf-idfの結果を結合\n",
        "lgb_train_concat_df = pd.concat([all_features_df, papers_data], axis=1)\n",
        "\n",
        "# lightGBMにて学習する際に不要なカラムを削除\n",
        "lgb_train_df = lgb_train_concat_df.drop([\"title\", \"Title_clean\", \"abstract\", \"Abstract_clean\", \"id\"], axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "uyF_gycfnu2z"
      },
      "outputs": [],
      "source": [
        "# 重複する列を削除する\n",
        "lgb_train_df = lgb_train_df.loc[:, ~lgb_train_df.columns.duplicated()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "APuZwNwlgGIa",
        "outputId": "e357d404-1f3c-4b07-b64f-3628df5c9e20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 10 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[100]\tvalid_0's binary_logloss: 0.000278031\n",
            "Accuracy: 1.0\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 訓練データとテストデータに分割\n",
        "X_train, X_test, y_train, y_test = train_test_split(lgb_train_df, lgb_train_df[\"judgement\"], test_size=0.2, random_state=42)\n",
        "\n",
        "# LightGBMのデータセットに変換\n",
        "train_data = lgb.Dataset(X_train, label=y_train)\n",
        "test_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\n",
        "\n",
        "# パラメータの設定\n",
        "params = {\n",
        "    'objective': 'binary',  # 2クラス分類の場合\n",
        "    'metric': 'binary_logloss',\n",
        "    'num_leaves': 31,\n",
        "    'learning_rate': 0.05,\n",
        "    'verbose': -1\n",
        "}\n",
        "\n",
        "# モデルの訓練\n",
        "verbose_eval = 0  # この数字を1にすると学習時のスコア推移がコマンドライン表示される\n",
        "bst = lgb.train(\n",
        "    params,\n",
        "    train_data,\n",
        "    valid_sets=[test_data],\n",
        "    num_boost_round=100,\n",
        "    callbacks=[lgb.early_stopping(stopping_rounds=10,\n",
        "                  verbose=True), # early_stopping用コールバック関数\n",
        "                  lgb.log_evaluation(verbose_eval)] # コマンドライン出力用コールバック関数\n",
        "    )\n",
        "\n",
        "# テストデータでの予測\n",
        "y_pred = bst.predict(X_test, num_iteration=bst.best_iteration)\n",
        "# 予測結果を二値（0または1）に変換\n",
        "y_pred_binary = np.where(y_pred > 0.5, 1, 0)\n",
        "\n",
        "# 精度の評価\n",
        "accuracy = accuracy_score(y_test, y_pred_binary)\n",
        "print(f'Accuracy: {accuracy}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyQI2VwUNz_g"
      },
      "source": [
        "# test_dfの分類"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWjd6ByAoTyr"
      },
      "source": [
        "## Pre-Process test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ij18Mx4koWWb",
        "outputId": "b872c600-2bf0-4e08-9d15-2de586e267ff"
      },
      "outputs": [],
      "source": [
        "# test_papers_data = test_df\n",
        "\n",
        "# test_papers_data['abstract'] = test_papers_data['abstract'].fillna('')\n",
        "\n",
        "# # clean text\n",
        "# test_papers_data['Title_clean'] = test_papers_data['title'].apply(lambda x:clean_text(x))\n",
        "# test_papers_data['Abstract_clean'] = test_papers_data['abstract'].apply(lambda x:clean_text(x))\n",
        "\n",
        "# # tf-idf\n",
        "# test_abstract2kw = extract_tfidf_keywords(test_papers_data['Abstract_clean'], 20)\n",
        "# test_title2kw = extract_tfidf_keywords(test_papers_data['Title_clean'], 20)\n",
        "\n",
        "# # 文書のベクトル表現を特徴量化する\n",
        "# test_abstract2kw_features_df = create_tfidf_features_df(test_papers_data['Abstract_clean'])\n",
        "# test_title2kw_features_df = create_tfidf_features_df(test_papers_data['Title_clean'])\n",
        "\n",
        "# test_doc2vecs = []\n",
        "# for i in range(len(test_abstract2kw)):\n",
        "#     vec = np.zeros(100)  # ベクトルの初期化をNumPy配列で行う\n",
        "#     for word in test_abstract2kw[i]:\n",
        "#         if word in word2vec_model.wv.key_to_index:  # 単語がモデルの語彙に含まれているかチェック\n",
        "#             vec += word2vec_model.wv[word]  # 単語ベクトルを加算\n",
        "\n",
        "#     for word in test_title2kw[i]:\n",
        "#         if word in word2vec_model.wv.key_to_index:  # 単語がモデルの語彙に含まれているかチェック\n",
        "#             vec += word2vec_model.wv[word]  # 単語ベクトルを加算\n",
        "\n",
        "#     test_doc2vecs.append(vec / (len(test_abstract2kw[i]) + len(test_title2kw[i])))  # 平均ベクトルを計算してリストに追加\n",
        "\n",
        "\n",
        "# # word2vecsの出力を特徴量として利用するためにDataframeにする\n",
        "# test_df_features = pd.DataFrame(test_doc2vecs)\n",
        "\n",
        "# # 全てのdataframeを結合する\n",
        "# test_all_features_df = pd.concat([test_df_features, test_abstract2kw_features_df, test_title2kw_features_df], axis=1)\n",
        "\n",
        "# # test_all_features_df.drop([\"title\", \"Title_clean\", \"abstract\", \"Abstract_clean\", \"id\"], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "8YDuoSPnLhZO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import linear_kernel\n",
        "\n",
        "# 模擬的な関数定義\n",
        "def clean_text(text):\n",
        "    # ここでは簡単なテキストクリーニングを想定\n",
        "    return text.lower().replace('[^a-zA-Z0-9]', ' ')\n",
        "\n",
        "def extract_tfidf_keywords(vectorizer, texts, n_keywords=20):\n",
        "    tfidf_matrix = vectorizer.transform(texts)\n",
        "    keywords_list = []\n",
        "    for row in tfidf_matrix:\n",
        "        row = row.toarray().flatten()\n",
        "        top_n_idxs = row.argsort()[-n_keywords:]\n",
        "        keywords = [vectorizer.get_feature_names_out()[i] for i in top_n_idxs]\n",
        "        keywords_list.append(keywords)\n",
        "    return keywords_list\n",
        "\n",
        "def create_tfidf_features_df(vectorizer, texts):\n",
        "    tfidf_matrix = vectorizer.transform(texts)\n",
        "    return pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "\n",
        "def compute_average_document_vectors(abstract_keywords, title_keywords, model):\n",
        "    doc_vectors = []\n",
        "    for abstract_kw, title_kw in zip(abstract_keywords, title_keywords):\n",
        "        vec = np.zeros(100)  # ベクトルの初期化\n",
        "        total_keywords = abstract_kw + title_kw  # abstractとtitleのキーワードを結合\n",
        "        for word in total_keywords:\n",
        "            if word in model.wv.key_to_index:\n",
        "                vec += model.wv[word]  # 単語ベクトルを加算\n",
        "        if len(total_keywords) > 0:  # ゼロ除算を避ける\n",
        "            vec = vec / len(total_keywords)  # 平均ベクトルを計算\n",
        "        doc_vectors.append(vec)\n",
        "    return pd.DataFrame(doc_vectors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "# テキストのクリーニング\n",
        "test_df['Title_clean'] = test_df['title'].apply(clean_text)\n",
        "test_df['Abstract_clean'] = test_df['abstract'].fillna('').apply(clean_text)\n",
        "\n",
        "# TF-IDFによるキーワード抽出\n",
        "test_abstract2kw = extract_tfidf_keywords(abstract2kw_vectorizer, test_df['Abstract_clean'])\n",
        "test_title2kw = extract_tfidf_keywords(title2kw_vectorizer, test_df['Title_clean'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\"\"\"_summary_\n",
        "文書（この場合は特定のテキストデータ、例えば学術論文の要旨やタイトルなど）の集合から、それぞれの文書を表すベクトルを計算しています。\n",
        "このベクトルは、文書に含まれるキーワード（単語）のword2vecモデルによるベクトル表現の平均を使って表されます。\n",
        "\"\"\"\n",
        "av_docs_vectors = compute_average_document_vectors(test_abstract2kw, test_title2kw, word2vec_model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TF-IDF特徴量のDataFrameを生成\n",
        "test_abstract2kw_features_df = create_tfidf_features_df(abstract2kw_vectorizer, test_df['Abstract_clean'])\n",
        "test_title2kw_features_df = create_tfidf_features_df(title2kw_vectorizer, test_df['Title_clean'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 全てのdataframeを結合\n",
        "test_all_features_df = pd.concat([av_docs_vectors, test_abstract2kw_features_df, test_title2kw_features_df], axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>imag</th>\n",
              "      <th>infect</th>\n",
              "      <th>pandem</th>\n",
              "      <th>patient</th>\n",
              "      <th>s</th>\n",
              "      <th>s diseas</th>\n",
              "      <th>sar</th>\n",
              "      <th>sar cov</th>\n",
              "      <th>studi</th>\n",
              "      <th>use</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.313450</td>\n",
              "      <td>-0.262022</td>\n",
              "      <td>-0.370631</td>\n",
              "      <td>-0.433546</td>\n",
              "      <td>-0.181466</td>\n",
              "      <td>0.322002</td>\n",
              "      <td>-0.244804</td>\n",
              "      <td>0.395894</td>\n",
              "      <td>-0.120175</td>\n",
              "      <td>-0.067032</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.720984</td>\n",
              "      <td>0.187725</td>\n",
              "      <td>-0.624572</td>\n",
              "      <td>-1.399040</td>\n",
              "      <td>-0.542386</td>\n",
              "      <td>0.527289</td>\n",
              "      <td>-0.172365</td>\n",
              "      <td>-0.048469</td>\n",
              "      <td>-0.073086</td>\n",
              "      <td>-0.303633</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.804634</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.593771</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.441122</td>\n",
              "      <td>-0.231897</td>\n",
              "      <td>-0.383284</td>\n",
              "      <td>-0.813188</td>\n",
              "      <td>-0.173573</td>\n",
              "      <td>-0.040248</td>\n",
              "      <td>0.021324</td>\n",
              "      <td>0.564605</td>\n",
              "      <td>-0.284627</td>\n",
              "      <td>0.083089</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.323778</td>\n",
              "      <td>-0.242939</td>\n",
              "      <td>-0.363021</td>\n",
              "      <td>-0.734906</td>\n",
              "      <td>-0.123075</td>\n",
              "      <td>-0.072503</td>\n",
              "      <td>0.067344</td>\n",
              "      <td>0.524283</td>\n",
              "      <td>-0.232844</td>\n",
              "      <td>0.103261</td>\n",
              "      <td>...</td>\n",
              "      <td>0.702299</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.489942</td>\n",
              "      <td>-0.024809</td>\n",
              "      <td>-0.453332</td>\n",
              "      <td>-0.631850</td>\n",
              "      <td>-0.396325</td>\n",
              "      <td>0.304810</td>\n",
              "      <td>-0.078264</td>\n",
              "      <td>0.443526</td>\n",
              "      <td>-0.083136</td>\n",
              "      <td>-0.201548</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40829</th>\n",
              "      <td>0.231887</td>\n",
              "      <td>-0.360788</td>\n",
              "      <td>-0.216855</td>\n",
              "      <td>-0.173814</td>\n",
              "      <td>-0.238057</td>\n",
              "      <td>0.294829</td>\n",
              "      <td>-0.329245</td>\n",
              "      <td>0.278282</td>\n",
              "      <td>-0.024866</td>\n",
              "      <td>-0.044752</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40830</th>\n",
              "      <td>0.404116</td>\n",
              "      <td>-0.021893</td>\n",
              "      <td>-0.568177</td>\n",
              "      <td>-0.565057</td>\n",
              "      <td>-0.267271</td>\n",
              "      <td>0.116456</td>\n",
              "      <td>-0.105392</td>\n",
              "      <td>0.576486</td>\n",
              "      <td>-0.067052</td>\n",
              "      <td>-0.185167</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.415273</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.590885</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40831</th>\n",
              "      <td>0.376252</td>\n",
              "      <td>-0.081933</td>\n",
              "      <td>-0.474475</td>\n",
              "      <td>-0.947627</td>\n",
              "      <td>-0.308826</td>\n",
              "      <td>0.018372</td>\n",
              "      <td>0.076142</td>\n",
              "      <td>0.390184</td>\n",
              "      <td>-0.222304</td>\n",
              "      <td>-0.044881</td>\n",
              "      <td>...</td>\n",
              "      <td>0.693962</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.720011</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40832</th>\n",
              "      <td>0.720984</td>\n",
              "      <td>0.187725</td>\n",
              "      <td>-0.624572</td>\n",
              "      <td>-1.399040</td>\n",
              "      <td>-0.542386</td>\n",
              "      <td>0.527289</td>\n",
              "      <td>-0.172365</td>\n",
              "      <td>-0.048469</td>\n",
              "      <td>-0.073086</td>\n",
              "      <td>-0.303633</td>\n",
              "      <td>...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40833</th>\n",
              "      <td>0.485204</td>\n",
              "      <td>-0.091262</td>\n",
              "      <td>-0.384573</td>\n",
              "      <td>-0.895363</td>\n",
              "      <td>-0.349399</td>\n",
              "      <td>-0.028954</td>\n",
              "      <td>-0.026244</td>\n",
              "      <td>0.487076</td>\n",
              "      <td>-0.335224</td>\n",
              "      <td>-0.125364</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>40834 rows × 448 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "              0         1         2         3         4         5         6  \\\n",
              "0      0.313450 -0.262022 -0.370631 -0.433546 -0.181466  0.322002 -0.244804   \n",
              "1      0.720984  0.187725 -0.624572 -1.399040 -0.542386  0.527289 -0.172365   \n",
              "2      0.441122 -0.231897 -0.383284 -0.813188 -0.173573 -0.040248  0.021324   \n",
              "3      0.323778 -0.242939 -0.363021 -0.734906 -0.123075 -0.072503  0.067344   \n",
              "4      0.489942 -0.024809 -0.453332 -0.631850 -0.396325  0.304810 -0.078264   \n",
              "...         ...       ...       ...       ...       ...       ...       ...   \n",
              "40829  0.231887 -0.360788 -0.216855 -0.173814 -0.238057  0.294829 -0.329245   \n",
              "40830  0.404116 -0.021893 -0.568177 -0.565057 -0.267271  0.116456 -0.105392   \n",
              "40831  0.376252 -0.081933 -0.474475 -0.947627 -0.308826  0.018372  0.076142   \n",
              "40832  0.720984  0.187725 -0.624572 -1.399040 -0.542386  0.527289 -0.172365   \n",
              "40833  0.485204 -0.091262 -0.384573 -0.895363 -0.349399 -0.028954 -0.026244   \n",
              "\n",
              "              7         8         9  ...      imag    infect  pandem  \\\n",
              "0      0.395894 -0.120175 -0.067032  ...  0.000000  0.000000     1.0   \n",
              "1     -0.048469 -0.073086 -0.303633  ...  0.000000  0.804634     0.0   \n",
              "2      0.564605 -0.284627  0.083089  ...  0.000000  0.000000     0.0   \n",
              "3      0.524283 -0.232844  0.103261  ...  0.702299  0.000000     0.0   \n",
              "4      0.443526 -0.083136 -0.201548  ...  0.000000  0.000000     0.0   \n",
              "...         ...       ...       ...  ...       ...       ...     ...   \n",
              "40829  0.278282 -0.024866 -0.044752  ...  0.000000  0.000000     0.0   \n",
              "40830  0.576486 -0.067052 -0.185167  ...  0.000000  0.000000     0.0   \n",
              "40831  0.390184 -0.222304 -0.044881  ...  0.693962  0.000000     0.0   \n",
              "40832 -0.048469 -0.073086 -0.303633  ...  1.000000  0.000000     0.0   \n",
              "40833  0.487076 -0.335224 -0.125364  ...  0.000000  0.000000     0.0   \n",
              "\n",
              "        patient    s  s diseas  sar  sar cov  studi       use  \n",
              "0      0.000000  0.0       0.0  0.0      0.0    0.0  0.000000  \n",
              "1      0.593771  0.0       0.0  0.0      0.0    0.0  0.000000  \n",
              "2      0.000000  0.0       0.0  0.0      0.0    0.0  0.000000  \n",
              "3      0.000000  0.0       0.0  0.0      0.0    0.0  0.000000  \n",
              "4      1.000000  0.0       0.0  0.0      0.0    0.0  0.000000  \n",
              "...         ...  ...       ...  ...      ...    ...       ...  \n",
              "40829  0.000000  0.0       0.0  0.0      0.0    1.0  0.000000  \n",
              "40830  0.415273  0.0       0.0  0.0      0.0    0.0  0.590885  \n",
              "40831  0.000000  0.0       0.0  0.0      0.0    0.0  0.720011  \n",
              "40832  0.000000  0.0       0.0  0.0      0.0    0.0  0.000000  \n",
              "40833  0.000000  0.0       0.0  0.0      0.0    0.0  0.000000  \n",
              "\n",
              "[40834 rows x 448 columns]"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_all_features_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "qKbBS_lvqnDc"
      },
      "outputs": [],
      "source": [
        "# テストデータでの予測\n",
        "y_pred = bst.predict(\n",
        "    test_all_features_df, \n",
        "    num_iteration=bst.best_iteration,\n",
        "    predict_disable_shape_check=True # lightGBMに使用されたデータと、test_all_features_dfのカラム数が違うのでエラーが出るため一旦無視する\n",
        ")\n",
        "# 予測結果を二値（0または1）に変換\n",
        "y_pred_binary = np.where(y_pred > 0.5, 0, 1)\n",
        "\n",
        "# 予測結果を DataFrame に変換\n",
        "y_pred_df = pd.DataFrame(y_pred_binary, columns=['prediction'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "# test_dfに予測結果を結合\n",
        "test_df['prediction'] = y_pred_binary\n",
        "\n",
        "# 最終的な提出ファイルの形式に合わせる\n",
        "final_submission = test_df[['id', 'prediction']]\n",
        "\n",
        "# CSVファイルとして保存\n",
        "final_submission.to_csv('submission/final_submission.csv', index=False, header=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "mEmzaJaau0ST"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "不足しているカラム:\n",
            "Abstract_clean\n",
            "Title_clean\n",
            "judgement\n",
            "title\n",
            "abstract\n",
            "id\n"
          ]
        }
      ],
      "source": [
        "# 学習データとテストデータのカラムセットを取得\n",
        "train_columns = set(train_df.columns)\n",
        "test_columns = set(test_all_features_df.columns)\n",
        "\n",
        "# テストデータに不足しているカラムを見つける\n",
        "missing_columns = train_columns - test_columns\n",
        "\n",
        "# 不足しているカラムを表示\n",
        "print(\"不足しているカラム:\")\n",
        "for column in missing_columns:\n",
        "    print(column)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
